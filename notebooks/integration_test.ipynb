{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XPECTO Competition System - Integration Testing\n",
    "\n",
    "This notebook tests the integration between the competition system and the epidemic engine, focusing on:\n",
    "1. Step callbacks functionality\n",
    "2. Metrics tracking\n",
    "3. Scoring system validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path to ensure imports work correctly\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to the path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import required modules\n",
    "from src.competition import CompetitionManager\n",
    "from src.mirage.engines.base import EngineV1\n",
    "from src.competition.services.simulation_integration import SimulationIntegration\n",
    "from src.competition.services.competition_service import CompetitionService\n",
    "from src.competition.data.storage import LocalStorageProvider\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure pandas to display more columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Step Callbacks\n",
    "\n",
    "First, we'll test if the engine correctly registers and executes step callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the engine\n",
    "engine = EngineV1()\n",
    "\n",
    "# Create a variable to track callback execution\n",
    "callback_counter = 0\n",
    "step_data = []\n",
    "\n",
    "# Define a test callback function\n",
    "def test_callback(step, state):\n",
    "    global callback_counter, step_data\n",
    "    callback_counter += 1\n",
    "    step_data.append(step)\n",
    "    print(f\"Callback executed for step {step}\")\n",
    "\n",
    "# Register the callback\n",
    "engine.register_step_callback(test_callback)\n",
    "\n",
    "# Run the engine for a few steps\n",
    "engine.run(steps=10)\n",
    "\n",
    "# Verify callbacks were executed\n",
    "print(f\"\\nCallback was executed {callback_counter} times\")\n",
    "print(f\"Steps recorded: {step_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Competition Manager and Test Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test data directory\n",
    "test_data_dir = \"test_data\"\n",
    "os.makedirs(test_data_dir, exist_ok=True)\n",
    "\n",
    "# Initialize competition manager with a new engine\n",
    "engine = EngineV1()\n",
    "competition = CompetitionManager(data_dir=test_data_dir, engine=engine)\n",
    "\n",
    "# Register test player\n",
    "player_id = competition.setup_player(name=\"Test Player\")\n",
    "\n",
    "# Select and display standard scenario\n",
    "competition.set_scenario(\"standard\")\n",
    "competition.display_scenario_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Metrics Tracking During Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the simulation\n",
    "competition.setup_simulation()\n",
    "\n",
    "# Define a basic intervention strategy for testing\n",
    "def test_intervention(engine):\n",
    "    print(\"Applying test intervention\")\n",
    "    # Set some basic parameters - these should be adapted to your actual engine API\n",
    "    if hasattr(engine, 'set_lockdown_level'):\n",
    "        engine.set_lockdown_level(0.5)\n",
    "    if hasattr(engine, 'allocate_resources'):\n",
    "        engine.allocate_resources('healthcare', 100)\n",
    "\n",
    "# Run simulation and capture results\n",
    "results = competition.run_simulation(steps=50, interventions=[test_intervention])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the raw metrics data\n",
    "if 'raw_metrics' in results and 'metrics_history' in results['raw_metrics']:\n",
    "    metrics_history = results['raw_metrics']['metrics_history']\n",
    "    print(f\"Collected {len(metrics_history)} data points from simulation\")\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    if metrics_history:\n",
    "        df = pd.DataFrame(metrics_history)\n",
    "        print(\"\\nMetrics dataframe head:\")\n",
    "        display(df.head())\n",
    "        \n",
    "        # Plot some metrics if available\n",
    "        if 'infected' in df.columns and 'step' in df.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(df['step'], df['infected'])\n",
    "            plt.title('Infection Progression')\n",
    "            plt.xlabel('Step')\n",
    "            plt.ylabel('Infected Population')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No metrics history found in results\")\n",
    "else:\n",
    "    print(\"Raw metrics or metrics history not found in results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Scoring System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the score breakdown from the simulation\n",
    "competition.display_score(results)\n",
    "\n",
    "# Create a storage provider and competition service for direct testing\n",
    "storage = LocalStorageProvider(data_dir=test_data_dir)\n",
    "service = CompetitionService(storage)\n",
    "\n",
    "# Test with some known values\n",
    "test_scores = [\n",
    "    # population_survived, gdp_preserved, infection_control, resource_efficiency, time_to_containment\n",
    "    (1.0, 1.0, 1.0, 1.0, 1.0),  # Perfect score\n",
    "    (0.9, 0.8, 0.7, 0.6, 0.5),  # Good score\n",
    "    (0.5, 0.5, 0.5, 0.5, 0.5),  # Average score\n",
    "    (0.3, 0.2, 0.1, 0.1, 0.0),  # Poor score\n",
    "    (0.0, 0.0, 0.0, 0.0, 0.0)   # Worst score\n",
    "]\n",
    "\n",
    "print(\"\\nScoring system tests with known values:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Population':^12} {'GDP':^12} {'Infection':^12} {'Resources':^12} {'Containment':^12} {'Final Score':^12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for scores in test_scores:\n",
    "    pop, gdp, inf, res, time = scores\n",
    "    final = service.calculate_score(pop, gdp, inf, res, time)\n",
    "    print(f\"{pop:^12.2f} {gdp:^12.2f} {inf:^12.2f} {res:^12.2f} {time:^12.2f} {final:^12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual score components\n",
    "test_case = {\n",
    "    \"population\": (10000, 1000),  # initial, dead\n",
    "    \"gdp\": (1000, 800),  # initial, final\n",
    "    \"infection_rate\": 0.25,  # max infection rate\n",
    "    \"resources\": 500,  # total resources spent\n",
    "    \"containment\": (250, 730)  # containment step, total steps\n",
    "}\n",
    "\n",
    "# Calculate individual components\n",
    "population_survived = service.calculate_population_survived_score(\n",
    "    test_case[\"population\"][0], test_case[\"population\"][1])\n",
    "gdp_preserved = service.calculate_gdp_preserved_score(\n",
    "    test_case[\"gdp\"][0], test_case[\"gdp\"][1])\n",
    "infection_control = service.calculate_infection_control_score(\n",
    "    test_case[\"infection_rate\"])\n",
    "resource_efficiency = service.calculate_resource_efficiency_score(\n",
    "    population_survived, infection_control, test_case[\"resources\"])\n",
    "time_to_containment = service.calculate_time_to_containment_score(\n",
    "    test_case[\"containment\"][0], test_case[\"containment\"][1])\n",
    "\n",
    "# Calculate final score\n",
    "final_score = service.calculate_score(\n",
    "    population_survived, \n",
    "    gdp_preserved,\n",
    "    infection_control,\n",
    "    resource_efficiency,\n",
    "    time_to_containment\n",
    ")\n",
    "\n",
    "print(\"\\nDetailed component scoring test:\")\n",
    "print(f\"Test case: {test_case}\")\n",
    "print(f\"Population Survived:   {population_survived:.4f} (weight: 0.30)\")\n",
    "print(f\"GDP Preserved:         {gdp_preserved:.4f} (weight: 0.20)\")\n",
    "print(f\"Infection Control:     {infection_control:.4f} (weight: 0.20)\")\n",
    "print(f\"Resource Efficiency:   {resource_efficiency:.4f} (weight: 0.15)\")\n",
    "print(f\"Time to Containment:   {time_to_containment:.4f} (weight: 0.15)\")\n",
    "print(f\"Final Score:           {final_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Player Attempts and Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display player attempts\n",
    "competition.display_player_attempts()\n",
    "\n",
    "# Run official attempt\n",
    "competition.toggle_practice_mode(is_practice=False)\n",
    "official_results = competition.run_simulation(steps=50, interventions=[test_intervention])\n",
    "\n",
    "# Display updated attempts\n",
    "competition.display_player_attempts()\n",
    "\n",
    "# Display leaderboard\n",
    "competition.display_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean Up Test Data\n",
    "\n",
    "Uncomment to remove test data directory when finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.rmtree(test_data_dir, ignore_errors=True)\n",
    "# print(f\"Removed test data directory: {test_data_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
